{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superposition Distillation: The \"Spoon Feed\" Protocol\n",
    "\n",
    "This notebook implements knowledge distillation via hidden state matching.\n",
    "\n",
    "**The Key Insight**: Force a small 1.58-bit model to reconstruct the high-dimensional\n",
    "hidden states of a large teacher. Because ternary weights are sparse/orthogonal,\n",
    "they can encode MORE features via superposition than their dimension suggests.\n",
    "\n",
    "**Protocol**:\n",
    "1. **Phase 1**: Cache teacher hidden states (then delete teacher to free VRAM)\n",
    "2. **Phase 2**: Train student to project UP to teacher's dimension space\n",
    "3. **Phase 3**: Re-train LM head to decode the new representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup: Install Dependencies\n",
    "!pip install -q torch transformers datasets bitsandbytes accelerate tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration\n",
    "# @markdown Adjust these based on your hardware\n",
    "\n",
    "TEACHER_MODEL = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"  # @param [\"Qwen/Qwen2.5-Coder-1.5B-Instruct\", \"Qwen/Qwen2.5-Coder-7B-Instruct\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"]\n",
    "DATASET = \"roneneldan/TinyStories\"  # @param [\"roneneldan/TinyStories\", \"Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B\"]\n",
    "\n",
    "# Data settings\n",
    "NUM_CACHE_SAMPLES = 500  # @param {type:\"slider\", min:100, max:2000, step:100}\n",
    "MAX_SEQ_LENGTH = 128  # @param {type:\"slider\", min:64, max:512, step:64}\n",
    "\n",
    "# Student settings\n",
    "STUDENT_DIM = 768  # @param {type:\"integer\"}\n",
    "STUDENT_LAYERS = 12  # @param {type:\"integer\"}\n",
    "STUDENT_HEADS = 12  # @param {type:\"integer\"}\n",
    "\n",
    "# Training settings\n",
    "SUPERPOSITION_STEPS = 500  # @param {type:\"slider\", min:100, max:2000, step:100}\n",
    "LM_HEAD_STEPS = 200  # @param {type:\"slider\", min:50, max:500, step:50}\n",
    "BATCH_SIZE = 16  # @param {type:\"slider\", min:4, max:64, step:4}\n",
    "LEARNING_RATE = 1e-3  # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Core Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# =============================================================================\n",
    "# BitNet Implementation\n",
    "# =============================================================================\n",
    "\n",
    "class STESign(torch.autograd.Function):\n",
    "    \"\"\"Straight-Through Estimator for ternary quantization.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.clamp(torch.round(x), -1, 1)\n",
    "    \n",
    "    @staticmethod  \n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output  # Pass gradient through\n",
    "\n",
    "\n",
    "class STERound(torch.autograd.Function):\n",
    "    \"\"\"STE for INT8 rounding.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.round(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    \"\"\"BitNet b1.58 Linear with proper quantization.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Quantize weights to {-1, 0, +1}\n",
    "        w_scale = self.weight.abs().mean() + 1e-8\n",
    "        w_quant = STESign.apply(self.weight / w_scale)\n",
    "        \n",
    "        # Quantize activations to INT8\n",
    "        a_scale = x.abs().amax(dim=-1, keepdim=True) + 1e-8\n",
    "        a_quant = STERound.apply(torch.clamp(x / a_scale * 127.0, -128, 127))\n",
    "        \n",
    "        # Compute and rescale\n",
    "        y = F.linear(a_quant, w_quant, None)\n",
    "        y = y * (w_scale * a_scale / 127.0)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Student Model\n",
    "# =============================================================================\n",
    "\n",
    "class BitNetBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with BitLinear.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int, mlp_ratio: float = 4.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        # Attention\n",
    "        self.q_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # MLP (SwiGLU)\n",
    "        mlp_dim = int(hidden_dim * mlp_ratio)\n",
    "        self.gate_proj = BitLinear(hidden_dim, mlp_dim)\n",
    "        self.up_proj = BitLinear(hidden_dim, mlp_dim)\n",
    "        self.down_proj = BitLinear(mlp_dim, hidden_dim)\n",
    "        \n",
    "        # Norms\n",
    "        self.input_norm = RMSNorm(hidden_dim)\n",
    "        self.post_attn_norm = RMSNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # Pre-norm attention\n",
    "        residual = x\n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        attn = attn.masked_fill(causal_mask, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, self.hidden_dim)\n",
    "        out = self.o_proj(out)\n",
    "        x = residual + out\n",
    "        \n",
    "        # Pre-norm MLP\n",
    "        residual = x\n",
    "        x = self.post_attn_norm(x)\n",
    "        x = self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class BitNetStudent(nn.Module):\n",
    "    \"\"\"BitNet student for superposition distillation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, hidden_dim: int = 768, \n",
    "                 num_layers: int = 12, num_heads: int = 12):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embed_tokens = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.layers = nn.ModuleList([BitNetBlock(hidden_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.norm = RMSNorm(hidden_dim)\n",
    "        \n",
    "        nn.init.normal_(self.embed_tokens.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, return_hidden_states: bool = True):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if return_hidden_states:\n",
    "            return x\n",
    "        else:\n",
    "            return F.linear(x, self.embed_tokens.weight)  # Tied LM head\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=0.7):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(input_ids, return_hidden_states=False)\n",
    "            next_logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Superposition Projector\n",
    "# =============================================================================\n",
    "\n",
    "class Decompressor(nn.Module):\n",
    "    \"\"\"Projects student hidden states to teacher's dimension.\"\"\"\n",
    "    def __init__(self, student_dim: int, teacher_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(student_dim, student_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(student_dim * 2),\n",
    "            nn.Linear(student_dim * 2, teacher_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "print(\"Core classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 1: Load Teacher & Cache Hidden States\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: Caching Teacher Hidden States\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {TEACHER_MODEL}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load teacher in 4-bit\n",
    "print(f\"Loading teacher model (4-bit)...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\n",
    "        TEACHER_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"4-bit failed: {e}\")\n",
    "    print(\"Trying FP16...\")\n",
    "    teacher = AutoModelForCausalLM.from_pretrained(\n",
    "        TEACHER_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "teacher.eval()\n",
    "TEACHER_DIM = teacher.config.hidden_size\n",
    "VOCAB_SIZE = teacher.config.vocab_size\n",
    "\n",
    "print(f\"Teacher hidden dim: {TEACHER_DIM}\")\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Load dataset\n",
    "print(f\"\\nLoading dataset: {DATASET}\")\n",
    "dataset = load_dataset(DATASET, split=\"train\", streaming=True)\n",
    "\n",
    "# Cache hidden states\n",
    "print(f\"\\nCaching {NUM_CACHE_SAMPLES} samples...\")\n",
    "cached_inputs = []\n",
    "cached_targets = []\n",
    "\n",
    "count = 0\n",
    "for item in tqdm(dataset, total=NUM_CACHE_SAMPLES):\n",
    "    if count >= NUM_CACHE_SAMPLES:\n",
    "        break\n",
    "    \n",
    "    # Get text\n",
    "    if 'text' in item:\n",
    "        text = item['text']\n",
    "    elif 'instruction' in item:\n",
    "        text = item['instruction']\n",
    "        if 'response' in item:\n",
    "            text = f\"{item['instruction']}\\n{item['response']}\"\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if not text or len(text) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get teacher hidden states\n",
    "    with torch.no_grad():\n",
    "        outputs = teacher(input_ids=inputs.input_ids, output_hidden_states=True)\n",
    "        hidden = outputs.hidden_states[-1].cpu()  # Last layer\n",
    "    \n",
    "    cached_inputs.append(inputs.input_ids.cpu())\n",
    "    cached_targets.append(hidden)\n",
    "    count += 1\n",
    "\n",
    "print(f\"\\nCached {len(cached_inputs)} samples\")\n",
    "print(f\"Input shape: {cached_inputs[0].shape}\")\n",
    "print(f\"Target shape: {cached_targets[0].shape}\")\n",
    "\n",
    "# Free teacher memory\n",
    "print(\"\\nUnloading teacher...\")\n",
    "del teacher\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"Teacher unloaded! VRAM freed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Create Student Model\n",
    "\n",
    "print(\"\\nCreating student model...\")\n",
    "student = BitNetStudent(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_dim=STUDENT_DIM,\n",
    "    num_layers=STUDENT_LAYERS,\n",
    "    num_heads=STUDENT_HEADS,\n",
    ").to(device)\n",
    "\n",
    "# Create projector\n",
    "projector = Decompressor(STUDENT_DIM, TEACHER_DIM).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in student.parameters())\n",
    "proj_params = sum(p.numel() for p in projector.parameters())\n",
    "\n",
    "print(f\"Student parameters: {total_params / 1e6:.1f}M\")\n",
    "print(f\"Projector parameters: {proj_params / 1e6:.1f}M\")\n",
    "print(f\"Student hidden dim: {STUDENT_DIM}\")\n",
    "print(f\"Teacher hidden dim: {TEACHER_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 2: Superposition Training\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: Superposition Training\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Training student to project UP to teacher's hidden space...\")\n",
    "\n",
    "# Optimizer for both student and projector\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(student.parameters()) + list(projector.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.0,  # No weight decay for BitNet!\n",
    ")\n",
    "\n",
    "student.train()\n",
    "projector.train()\n",
    "\n",
    "num_samples = len(cached_inputs)\n",
    "losses = []\n",
    "\n",
    "progress = tqdm(range(SUPERPOSITION_STEPS), desc=\"Superposition\")\n",
    "for step in progress:\n",
    "    # Random batch\n",
    "    indices = torch.randint(0, num_samples, (BATCH_SIZE,))\n",
    "    \n",
    "    # Get batch\n",
    "    batch_in = torch.stack([cached_inputs[i] for i in indices])\n",
    "    batch_target = torch.stack([cached_targets[i] for i in indices])\n",
    "    \n",
    "    # Squeeze extra dimensions if present\n",
    "    if batch_in.dim() == 3 and batch_in.size(1) == 1:\n",
    "        batch_in = batch_in.squeeze(1)\n",
    "    if batch_target.dim() == 4 and batch_target.size(1) == 1:\n",
    "        batch_target = batch_target.squeeze(1)\n",
    "    \n",
    "    batch_in = batch_in.to(device)\n",
    "    batch_target = batch_target.to(device).float()\n",
    "    \n",
    "    # Forward: student -> projector\n",
    "    student_hidden = student(batch_in, return_hidden_states=True)\n",
    "    projected = projector(student_hidden)\n",
    "    \n",
    "    # Cosine similarity loss (more stable than MSE)\n",
    "    proj_flat = projected.view(-1, TEACHER_DIM)\n",
    "    target_flat = batch_target.view(-1, TEACHER_DIM)\n",
    "    \n",
    "    # We want cosine similarity = 1 (same direction)\n",
    "    ones = torch.ones(proj_flat.size(0), device=device)\n",
    "    loss = F.cosine_embedding_loss(proj_flat, target_flat, ones)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        avg_loss = sum(losses[-10:]) / len(losses[-10:])\n",
    "        progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "print(f\"\\nFinal superposition loss: {sum(losses[-50:]) / 50:.4f}\")\n",
    "\n",
    "# Plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cosine Embedding Loss')\n",
    "plt.title('Superposition Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Phase 3: LM Head Training (Speech Therapy)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 3: LM Head Training\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Re-training output head to decode new representations...\")\n",
    "\n",
    "# Freeze everything except embeddings (which are tied to LM head)\n",
    "for param in student.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze embedding (tied to LM head)\n",
    "student.embed_tokens.weight.requires_grad = True\n",
    "\n",
    "head_optimizer = torch.optim.AdamW(\n",
    "    [student.embed_tokens.weight],\n",
    "    lr=5e-3,  # Higher LR for head training\n",
    ")\n",
    "\n",
    "student.train()\n",
    "lm_losses = []\n",
    "\n",
    "progress = tqdm(range(LM_HEAD_STEPS), desc=\"LM Head\")\n",
    "for step in progress:\n",
    "    indices = torch.randint(0, num_samples, (BATCH_SIZE,))\n",
    "    \n",
    "    batch_in = torch.stack([cached_inputs[i] for i in indices])\n",
    "    if batch_in.dim() == 3 and batch_in.size(1) == 1:\n",
    "        batch_in = batch_in.squeeze(1)\n",
    "    batch_in = batch_in.to(device)\n",
    "    \n",
    "    # Forward with logits\n",
    "    logits = student(batch_in, return_hidden_states=False)\n",
    "    \n",
    "    # Next token prediction loss\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = batch_in[:, 1:].contiguous()\n",
    "    \n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, VOCAB_SIZE),\n",
    "        shift_labels.view(-1),\n",
    "    )\n",
    "    \n",
    "    head_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    head_optimizer.step()\n",
    "    \n",
    "    lm_losses.append(loss.item())\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        avg_loss = sum(lm_losses[-10:]) / len(lm_losses[-10:])\n",
    "        progress.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "# Unfreeze all for future training\n",
    "for param in student.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"\\nFinal LM loss: {sum(lm_losses[-20:]) / 20:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(lm_losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('LM Head Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Model\n",
    "\n",
    "print(\"Saving model...\")\n",
    "torch.save({\n",
    "    'student': student.state_dict(),\n",
    "    'projector': projector.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'hidden_dim': STUDENT_DIM,\n",
    "        'num_layers': STUDENT_LAYERS,\n",
    "        'num_heads': STUDENT_HEADS,\n",
    "        'teacher_dim': TEACHER_DIM,\n",
    "    }\n",
    "}, 'superposition_student.pt')\n",
    "\n",
    "print(\"Saved to 'superposition_student.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test Generation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "student.eval()\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The capital of France is\",\n",
    "    \"def fibonacci(n):\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = student.generate(input_ids, max_new_tokens=30, temperature=0.7)\n",
    "    \n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Output: {output_text}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements **Superposition Distillation**:\n",
    "\n",
    "1. **Phase 1**: Cache teacher's hidden states (then delete teacher)\n",
    "2. **Phase 2**: Train student to project UP to teacher's dimension\n",
    "3. **Phase 3**: Re-train LM head to decode new representations\n",
    "\n",
    "**Key insights**:\n",
    "- 1.58-bit weights are naturally sparse/orthogonal\n",
    "- Sparse vectors can encode MORE features via superposition\n",
    "- Cosine loss is more stable than MSE for hidden state matching\n",
    "- LM head needs retraining after superposition training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
