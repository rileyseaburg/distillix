{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitNet b1.58 Training - Microsoft Architecture\n",
    "\n",
    "This notebook implements **proper BitNet training** following Microsoft's approach from:\n",
    "- [BitNet b1.58 Paper](https://arxiv.org/abs/2402.17764)\n",
    "- [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)\n",
    "- [Microsoft BitNet GitHub](https://github.com/microsoft/BitNet)\n",
    "\n",
    "## Key Architecture Differences from Standard LLMs\n",
    "\n",
    "| Component | **Microsoft BitNet** | **Standard LLaMA** |\n",
    "|-----------|---------------------|-------------------|\n",
    "| FFN Activation | **Squared ReLU (ReLU²)** | SwiGLU |\n",
    "| Normalization | **SubLN** (post-norm after residual) | Pre-norm only |\n",
    "| Bias Terms | **None** | Optional |\n",
    "| Weight Decay | **0.0** (none!) | 0.01-0.1 |\n",
    "| Weight Quantization | Ternary {-1, 0, +1} via absmean | Full precision |\n",
    "| Activation Quantization | INT8 per-token | Full precision |\n",
    "\n",
    "## Training Pipeline\n",
    "1. **Pre-training**: Large-scale next-token prediction\n",
    "2. **SFT**: Supervised fine-tuning on instruction data\n",
    "3. **DPO** (optional): Direct preference optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install Dependencies\n",
    "!pip install -q torch transformers datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Configuration\n",
    "# @markdown Model size preset\n",
    "MODEL_SIZE = \"125M\"  # @param [\"125M\", \"350M\", \"1B\", \"3B\"]\n",
    "\n",
    "# @markdown Training settings\n",
    "MAX_STEPS = 10000  # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 8  # @param {type:\"integer\"}\n",
    "MAX_SEQ_LENGTH = 512  # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 1e-3  # @param {type:\"number\"}\n",
    "\n",
    "# @markdown Data source\n",
    "DATA_SOURCE = \"cognitive_kernel\"  # @param [\"cognitive_kernel\", \"swe_rebench\", \"huggingface\"]\n",
    "HF_DATASET = \"roneneldan/TinyStories\"  # @param {type:\"string\"}\n",
    "\n",
    "# Model configs\n",
    "MODEL_CONFIGS = {\n",
    "    \"125M\": {\"hidden_dim\": 768, \"num_layers\": 12, \"num_heads\": 12, \"mlp_ratio\": 4},\n",
    "    \"350M\": {\"hidden_dim\": 1024, \"num_layers\": 24, \"num_heads\": 16, \"mlp_ratio\": 4},\n",
    "    \"1B\": {\"hidden_dim\": 2048, \"num_layers\": 24, \"num_heads\": 16, \"mlp_ratio\": 4},\n",
    "    \"3B\": {\"hidden_dim\": 2560, \"num_layers\": 32, \"num_heads\": 20, \"mlp_ratio\": 4},\n",
    "}\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_SIZE]\n",
    "print(f\"Model config: {MODEL_SIZE}\")\n",
    "print(f\"  Hidden dim: {config['hidden_dim']}\")\n",
    "print(f\"  Layers: {config['num_layers']}\")\n",
    "print(f\"  Heads: {config['num_heads']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Core BitNet Implementation (Microsoft-style)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Straight-Through Estimators for Quantization\n",
    "# =============================================================================\n",
    "\n",
    "class STETernary(torch.autograd.Function):\n",
    "    \"\"\"STE for ternary quantization to {-1, 0, +1}.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.clamp(torch.round(x), -1, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output  # Straight-through: pass gradient unchanged\n",
    "\n",
    "\n",
    "class STERound(torch.autograd.Function):\n",
    "    \"\"\"STE for INT8 rounding.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return torch.round(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BitLinear - The Core BitNet Layer\n",
    "# =============================================================================\n",
    "\n",
    "class BitLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    BitNet b1.58 Linear Layer.\n",
    "    \n",
    "    Key differences from nn.Linear:\n",
    "    - Weights quantized to {-1, 0, +1} using absmean scaling\n",
    "    - Activations quantized to INT8 using absmax scaling (per-token)\n",
    "    - NO BIAS (Microsoft BitNet has no bias anywhere)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Latent weights in FP32 (updated by optimizer)\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        \n",
    "        # NO BIAS - critical for BitNet!\n",
    "        self.register_parameter('bias', None)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # === Weight Quantization (absmean scaling) ===\n",
    "        # Scale = mean(|W|) - better than max for ternary\n",
    "        w_scale = self.weight.abs().mean() + 1e-8\n",
    "        w_normalized = self.weight / w_scale\n",
    "        w_quant = STETernary.apply(w_normalized)  # {-1, 0, +1}\n",
    "        \n",
    "        # === Activation Quantization (absmax per-token) ===\n",
    "        a_scale = x.abs().amax(dim=-1, keepdim=True) + 1e-8\n",
    "        a_normalized = x / a_scale * 127.0\n",
    "        a_quant = STERound.apply(torch.clamp(a_normalized, -128, 127))\n",
    "        \n",
    "        # === Compute and Rescale ===\n",
    "        y = F.linear(a_quant, w_quant, None)\n",
    "        y = y * (w_scale * a_scale / 127.0)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RMSNorm - Root Mean Square Normalization\n",
    "# =============================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMSNorm without bias (Microsoft BitNet style).\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        # NO BIAS\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return (x / rms) * self.weight\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Rotary Position Embeddings (RoPE)\n",
    "# =============================================================================\n",
    "\n",
    "def precompute_freqs_cis(dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "    \"\"\"Precompute rotary embedding frequencies.\"\"\"\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(max_seq_len)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    \"\"\"Apply rotary embeddings to queries and keys.\"\"\"\n",
    "    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    \n",
    "    freqs_cis = freqs_cis[:xq.shape[1]].unsqueeze(0).unsqueeze(2)\n",
    "    \n",
    "    xq_out = torch.view_as_real(xq_complex * freqs_cis).flatten(-2)\n",
    "    xk_out = torch.view_as_real(xk_complex * freqs_cis).flatten(-2)\n",
    "    \n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# BitNet Transformer Block with SubLN and ReLU²\n",
    "# =============================================================================\n",
    "\n",
    "class BitNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    BitNet Transformer Block following Microsoft architecture:\n",
    "    - SubLN (post-norm after residual)\n",
    "    - Squared ReLU (ReLU²) activation instead of SwiGLU\n",
    "    - No bias anywhere\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        max_seq_len: int = 4096,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.mlp_dim = int(hidden_dim * mlp_ratio)\n",
    "        \n",
    "        # === Attention ===\n",
    "        self.q_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = BitLinear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # === FFN with Squared ReLU ===\n",
    "        # Microsoft uses ReLU² instead of SwiGLU for BitNet!\n",
    "        self.up_proj = BitLinear(hidden_dim, self.mlp_dim)\n",
    "        self.down_proj = BitLinear(self.mlp_dim, hidden_dim)\n",
    "        \n",
    "        # === SubLN (post-norm after residual) ===\n",
    "        # This is different from standard pre-norm!\n",
    "        self.attn_norm = RMSNorm(hidden_dim)\n",
    "        self.ffn_norm = RMSNorm(hidden_dim)\n",
    "        \n",
    "        # RoPE\n",
    "        self.register_buffer(\n",
    "            \"freqs_cis\",\n",
    "            precompute_freqs_cis(self.head_dim, max_seq_len),\n",
    "            persistent=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # === Attention ===\n",
    "        q = self.q_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Apply RoPE\n",
    "        q, k = apply_rotary_emb(q, k, self.freqs_cis.to(x.device))\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scale = 1.0 / math.sqrt(self.head_dim)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
    "        attn = attn.masked_fill(causal_mask, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, self.hidden_dim)\n",
    "        out = self.o_proj(out)\n",
    "        \n",
    "        # === SubLN: Add residual THEN normalize ===\n",
    "        x = self.attn_norm(x + out)\n",
    "        \n",
    "        # === FFN with Squared ReLU ===\n",
    "        # ReLU²(x) = ReLU(x)² - simpler and works better with ternary weights!\n",
    "        ffn_out = self.up_proj(x)\n",
    "        ffn_out = F.relu(ffn_out) ** 2  # Squared ReLU!\n",
    "        ffn_out = self.down_proj(ffn_out)\n",
    "        \n",
    "        # === SubLN again ===\n",
    "        x = self.ffn_norm(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Full BitNet Model\n",
    "# =============================================================================\n",
    "\n",
    "class BitNetLM(nn.Module):\n",
    "    \"\"\"BitNet Language Model following Microsoft architecture.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_dim: int = 768,\n",
    "        num_layers: int = 12,\n",
    "        num_heads: int = 12,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        max_seq_len: int = 4096,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Token embeddings (full precision)\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            BitNetBlock(hidden_dim, num_heads, mlp_ratio, max_seq_len)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final norm\n",
    "        self.norm = RMSNorm(hidden_dim)\n",
    "        \n",
    "        # LM head (tied to embeddings)\n",
    "        self.lm_head = None  # Will use embed_tokens.weight\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.embed_tokens.weight, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Tied LM head\n",
    "        logits = F.linear(x, self.embed_tokens.weight)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, input_ids: torch.Tensor, max_new_tokens: int = 50, temperature: float = 0.7):\n",
    "        \"\"\"Simple greedy/sampling generation.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits = self.forward(input_ids)\n",
    "                next_logits = logits[:, -1, :] / temperature\n",
    "                probs = F.softmax(next_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        return input_ids\n",
    "    \n",
    "    def num_parameters(self) -> int:\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "print(\"BitNet architecture defined successfully!\")\n",
    "print(\"Key features:\")\n",
    "print(\"  - Squared ReLU (ReLU²) activation\")\n",
    "print(\"  - SubLN (post-norm after residual)\")\n",
    "print(\"  - No bias anywhere\")\n",
    "print(\"  - Absmean weight quantization\")\n",
    "print(\"  - Per-token INT8 activation quantization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data Loading\n",
    "\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class MultiFormatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset supporting multiple formats:\n",
    "    - cognitive_kernel: {\"prompt\": ..., \"response\": ..., \"type\": \"cognitive_kernel\"}\n",
    "    - swe_rebench: {\"prompt\": ..., \"response\": ...}\n",
    "    - plain: {\"text\": ...}\n",
    "    - messages: {\"messages\": [{\"role\": ..., \"content\": ...}, ...]}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, paths: list, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "        \n",
    "        for path in paths:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Warning: {path} not found\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Loading: {path}\")\n",
    "            with open(path, 'r') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        item = json.loads(line.strip())\n",
    "                        text = self._extract_text(item)\n",
    "                        if text and len(text) > 20:\n",
    "                            self.samples.append(text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} samples\")\n",
    "    \n",
    "    def _extract_text(self, item: dict) -> str:\n",
    "        \"\"\"Extract training text from various formats.\"\"\"\n",
    "        # Cognitive kernel format\n",
    "        if 'prompt' in item and 'response' in item:\n",
    "            return f\"### Instruction:\\n{item['prompt']}\\n\\n### Response:\\n{item['response']}\"\n",
    "        \n",
    "        # Plain text\n",
    "        if 'text' in item:\n",
    "            return item['text']\n",
    "        \n",
    "        # Messages format\n",
    "        if 'messages' in item:\n",
    "            parts = []\n",
    "            for msg in item['messages']:\n",
    "                role = msg.get('role', 'user')\n",
    "                content = msg.get('content', '')\n",
    "                parts.append(f\"### {role.title()}:\\n{content}\")\n",
    "            return \"\\n\\n\".join(parts)\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.samples[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        return {'input_ids': input_ids}\n",
    "\n",
    "\n",
    "def load_from_huggingface(dataset_name: str, tokenizer, max_length: int, num_samples: int = 10000):\n",
    "    \"\"\"Load dataset from HuggingFace.\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(f\"Loading from HuggingFace: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "    \n",
    "    samples = []\n",
    "    for i, item in enumerate(tqdm(dataset, total=num_samples)):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        \n",
    "        # Get text\n",
    "        if 'text' in item:\n",
    "            text = item['text']\n",
    "        elif 'content' in item:\n",
    "            text = item['content']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if text and len(text) > 20:\n",
    "            samples.append(text)\n",
    "    \n",
    "    print(f\"Loaded {len(samples)} samples from HuggingFace\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "print(\"Data loading utilities defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Tokenizer and Create Model\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use LLaMA-3 tokenizer (128k vocab) like Microsoft\n",
    "# Fall back to LLaMA-2 if not available\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    print(\"Loaded LLaMA-3 tokenizer\")\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "    print(\"Loaded LLaMA-2 tokenizer (fallback)\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Create model\n",
    "model = BitNetLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    num_layers=config['num_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_ratio=config['mlp_ratio'],\n",
    "    max_seq_len=MAX_SEQ_LENGTH * 2,\n",
    ").to(device)\n",
    "\n",
    "num_params = model.num_parameters()\n",
    "print(f\"\\nModel created: {num_params / 1e6:.1f}M parameters\")\n",
    "print(f\"Memory footprint: {num_params * 4 / 1e9:.2f} GB (FP32 training)\")\n",
    "print(f\"Inference footprint: {num_params * 1.58 / 8 / 1e9:.2f} GB (1.58-bit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load Training Data\n",
    "\n",
    "# Define data paths based on source\n",
    "if DATA_SOURCE == \"cognitive_kernel\":\n",
    "    data_paths = [\n",
    "        \"../data/distillation/cognitive_kernel_v2.jsonl\",\n",
    "        \"../data/distillation/train_full_10k.jsonl\",\n",
    "    ]\n",
    "    dataset = MultiFormatDataset(data_paths, tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "elif DATA_SOURCE == \"swe_rebench\":\n",
    "    data_paths = [\n",
    "        \"../data/distillation/swe_rebench_verified.jsonl\",\n",
    "        \"../data/distillation/swe_rebench_10k.jsonl\",\n",
    "    ]\n",
    "    dataset = MultiFormatDataset(data_paths, tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "else:  # HuggingFace\n",
    "    samples = load_from_huggingface(HF_DATASET, tokenizer, MAX_SEQ_LENGTH)\n",
    "    \n",
    "    # Create simple dataset\n",
    "    class SimpleDataset(Dataset):\n",
    "        def __init__(self, samples, tokenizer, max_length):\n",
    "            self.samples = samples\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            tokens = self.tokenizer(\n",
    "                self.samples[idx],\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            return {'input_ids': tokens['input_ids'].squeeze(0)}\n",
    "    \n",
    "    dataset = SimpleDataset(samples, tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dataset)} samples\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Training Loop (Microsoft-style: NO WEIGHT DECAY!)\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === CRITICAL: NO WEIGHT DECAY ===\n",
    "# Microsoft uses weight_decay=0.0 for BitNet!\n",
    "# Weight decay + ternary quantization = weight collapse\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0.0,  # CRITICAL: No weight decay!\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "def get_lr(step, warmup_steps=100, max_steps=MAX_STEPS):\n",
    "    if step < warmup_steps:\n",
    "        return LEARNING_RATE * step / warmup_steps\n",
    "    # Cosine decay\n",
    "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    return LEARNING_RATE * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "\n",
    "def check_weight_stats(model):\n",
    "    \"\"\"Check weight statistics to detect collapse.\"\"\"\n",
    "    stats = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.dim() >= 2:\n",
    "            stats[name] = {\n",
    "                'std': param.std().item(),\n",
    "                'abs_mean': param.abs().mean().item(),\n",
    "            }\n",
    "    \n",
    "    # Check for collapse (std < 0.001)\n",
    "    collapsed = [k for k, v in stats.items() if v['std'] < 0.001]\n",
    "    healthy = [k for k, v in stats.items() if v['std'] >= 0.001]\n",
    "    \n",
    "    return {\n",
    "        'collapsed': len(collapsed),\n",
    "        'healthy': len(healthy),\n",
    "        'total': len(stats),\n",
    "        'avg_std': sum(v['std'] for v in stats.values()) / len(stats) if stats else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "# Training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING BITNET (Microsoft-style)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Steps: {MAX_STEPS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight decay: 0.0 (CRITICAL for BitNet!)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "data_iter = iter(dataloader)\n",
    "start_time = time.time()\n",
    "\n",
    "progress = tqdm(range(MAX_STEPS), desc=\"Training\")\n",
    "for step in progress:\n",
    "    # Get batch\n",
    "    try:\n",
    "        batch = next(data_iter)\n",
    "    except StopIteration:\n",
    "        data_iter = iter(dataloader)\n",
    "        batch = next(data_iter)\n",
    "    \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Forward\n",
    "    logits = model(input_ids)\n",
    "    \n",
    "    # Compute loss (next token prediction)\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits.view(-1, VOCAB_SIZE),\n",
    "        shift_labels.view(-1),\n",
    "        ignore_index=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Logging\n",
    "    if (step + 1) % 50 == 0:\n",
    "        avg_loss = sum(losses[-50:]) / len(losses[-50:])\n",
    "        health = check_weight_stats(model)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        progress.set_postfix({\n",
    "            'loss': f\"{avg_loss:.4f}\",\n",
    "            'lr': f\"{lr:.2e}\",\n",
    "            'health': f\"{health['healthy']}/{health['total']}\",\n",
    "        })\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Final loss: {sum(losses[-100:]) / 100:.4f}\")\n",
    "\n",
    "# Check final weight health\n",
    "final_health = check_weight_stats(model)\n",
    "print(f\"Weight health: {final_health['healthy']}/{final_health['total']} healthy\")\n",
    "print(f\"Average std: {final_health['avg_std']:.6f}\")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Smoothed\n",
    "plt.subplot(1, 2, 2)\n",
    "window = 50\n",
    "smoothed = [sum(losses[max(0,i-window):i+1]) / min(i+1, window) for i in range(len(losses))]\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Smoothed Loss')\n",
    "plt.title('Smoothed Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Test Generation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"def fibonacci(n):\",\n",
    "    \"### Instruction:\\nWrite a function to reverse a string.\\n\\n### Response:\",\n",
    "    \"The quick brown fox\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=50, temperature=0.7)\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save Model\n",
    "\n",
    "save_path = f\"bitnet_{MODEL_SIZE}_step{MAX_STEPS}.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'step': MAX_STEPS,\n",
    "    'final_loss': sum(losses[-100:]) / 100 if losses else 0,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# Download link (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(save_path)\n",
    "except:\n",
    "    print(\"(Not in Colab, skipping download)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements **proper BitNet training** following Microsoft's architecture:\n",
    "\n",
    "### Key Differences from Standard LLMs\n",
    "\n",
    "1. **Squared ReLU (ReLU²)** instead of SwiGLU\n",
    "   - Simpler gradient flow\n",
    "   - Better sparsity with ternary weights\n",
    "\n",
    "2. **SubLN** (post-norm after residual) instead of pre-norm\n",
    "   - `x = Norm(x + Attn(x))` instead of `x = x + Attn(Norm(x))`\n",
    "   - Critical for training stability\n",
    "\n",
    "3. **No bias anywhere**\n",
    "   - Cleaner gradient flow\n",
    "   - Reduces parameters\n",
    "\n",
    "4. **ZERO weight decay**\n",
    "   - Weight decay + ternary quantization = weight collapse!\n",
    "   - This is the most critical difference\n",
    "\n",
    "### Training Pipeline\n",
    "\n",
    "For production models, Microsoft uses:\n",
    "1. **Pre-training**: 4T tokens, next-token prediction\n",
    "2. **SFT**: Instruction tuning with sum loss aggregation\n",
    "3. **DPO**: Alignment with human preferences\n",
    "\n",
    "### References\n",
    "\n",
    "- [BitNet Paper](https://arxiv.org/abs/2402.17764)\n",
    "- [BitNet 2B4T Report](https://arxiv.org/abs/2504.12285)\n",
    "- [Microsoft BitNet GitHub](https://github.com/microsoft/BitNet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
