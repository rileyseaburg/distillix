# Distillix - BitNet b1.58 Knowledge Distillation Framework
# Hardware: RTX 2080 Super (8GB) | Threadripper 3960X | 30GB RAM

# Core ML
torch>=2.1.0
torchvision>=0.16.0

# Transformers & Tokenization
transformers>=4.36.0
tokenizers>=0.15.0
sentencepiece>=0.1.99

# Data
datasets>=2.16.0
jsonlines>=4.0.0

# Training utilities
tqdm>=4.66.0
wandb>=0.16.0
tensorboard>=2.15.0

# HTTP client for OpenCode server
aiohttp>=3.9.0
httpx>=0.26.0

# Configuration
pyyaml>=6.0.1
pydantic>=2.5.0

# Numerical
numpy>=1.24.0
scipy>=1.11.0

# Utilities
rich>=13.7.0
typer>=0.9.0
python-dotenv>=1.0.0

# Memory optimization
einops>=0.7.0

# Safe serialization (faster + more secure than pickle)
safetensors>=0.4.0

# Optional: Flash Attention (if supported)
# flash-attn>=2.4.0  # Requires CUDA 11.8+
